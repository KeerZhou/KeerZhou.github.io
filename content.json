{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/08/21/hello-world/"},{"title":"我的第一篇博客文章","text":"第一章内容 第二章内容","link":"/2019/08/22/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%E6%96%87%E7%AB%A0/"},{"title":"Python拉勾网爬虫并存储到数据库","text":"拉勾网爬虫解析拉勾网网站： 在拉勾网上输入关键词后我们可以得到相应的岗位信息（这里以Python为例），我们先获取到网站中所有的城市信息，再通过城市信息遍历爬取全国的Python职位信息。 在数据包的Headers中我们可以得到网页头的相关信息，如网页URL、请求方法、Cookies信息、用户代理等相关信息。 获取所有城市： 123456789101112131415161718192021222324252627282930313233class CrawlLaGou(object): def __init__(self): # 使用session保存cookies信息 self.lagou_session = requests.session() self.header = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)' } self.city_list = \"\" #获取城市 def crawl_city(self): #使用正则表达式获取HTML代码中的城市名称 city_search = re.compile(r'www\\.lagou\\.com\\/.*\\/\"&gt;(.*?)&lt;/a&gt;') #网页URL city_url = \"https://www.lagou.com/jobs/allCity.html\" city_result = self.crawl_request(method=\"GET\", url=city_url) self.city_list = city_search.findall(city_result) self.lagou_session.cookies.clear() #返回结果 def crawl_request(self,method,url,data=None,info=None): while True: if method == \"GET\": response = self.lagou_session.get(url=url,headers=self.header) elif method == \"POST\": response = self.lagou_session.post(url=url, headers=self.header, data=data) response.encoding = \"utf8\" return response.textif __name__ == '__main__': lagou = CrawlLaGou() lagou.crawl_city() print(lagou.city_list) 其中self.header中的User-Agent信息也在上图中Headers中可以找到。上述代码先将url所对应的网页源码爬取下来，再通过正则表达式获取到网页中的所有城市名称。 运行结果： 在我们获取完所有的城市名称信息后，我们开始获取城市对应的职位信息，我们回到职位列表：https://www.lagou.com/jobs/list_python找到存放有职位信息的数据包，以及其对应的请求头部信息。 存放职位信息的数据包： 在得到网页的职位信息后，我们可以使用https://www.json.cn/进行解析，并找出我们需要的信息内容。 从json解析中，我们可以得到职位信息的列表为’content’→’positionResult’→’result’ 获取职位信息： 123456789101112131415161718192021222324252627282930#获取职位信息def crawl_city_job(self,city): #职位列表数据包的url first_request_url = \"https://www.lagou.com/jobs/list_python?city=%s&amp;cl=false&amp;fromSearch=true&amp;labelWords=&amp;suginput=\"%city first_response = self.crawl_request(method=\"GET\", url=first_request_url) #使用正则表达式获取职位列表的页数 total_page_search = re.compile(r'class=\"span\\stotalNum\"&gt;(\\d+)&lt;/span&gt;') try: total_page = total_page_search.search(first_response).group(1) except: # 如果没有职位信息，直接return return else: for i in range(1, int(total_page) + 1): #data信息中的字段 data = { \"pn\":i, \"kd\":\"python\" } #存放职位信息的url page_url = \"https://www.lagou.com/jobs/positionAjax.json?city=%s&amp;needAddtionalResult=false\" % city #添加对应的Referer referer_url = \"https://www.lagou.com/jobs/list_python?city=%s&amp;cl=false&amp;fromSearch=true&amp;labelWords=&amp;suginput=\"% city self.header['Referer'] = referer_url.encode() response = self.crawl_request(method=\"POST\",url=page_url,data=data,info=city) lagou_data = json.loads(response) #通过json解析得到的职位信息存放的列表 job_list = lagou_data['content']['positionResult']['result'] for job in job_list: print(job） 在上述代码中，先通过存放职位列表的数据包url（first_request_url）中获取网页代码中的页码信息，并通过页码来判断是否存在岗位信息，若没有则返回。若有，则通过存放职位信息的数据包url（page_url），并添加对应的data数据和Refer信息，来获取该数据包中的所有信息，最后通过’content’→’positionResult’→’result’的列表顺序来获得到我们所需要的职位信息。 运行结果： 解决“操作太频繁，请稍后再试”的问题：如在爬虫运行过程中出现“操作太频繁”则说明该爬虫已经被网站发现，此时我们需要清除cookies信息并重新获取该url，并让程序停止10s后再继续运行。 1234567891011121314151617#返回结果def crawl_request(self,method,url,data=None,info=None): while True: if method == \"GET\": response = self.lagou_session.get(url=url,headers=self.header) elif method == \"POST\": response = self.lagou_session.post(url=url, headers=self.header, data=data) response.encoding = \"utf8\" #解决操作太频繁问题 if '频繁' in response.text: print(response.text) self.lagou_session.cookies.clear() first_request_url = \"https://www.lagou.com/jobs/list_python?city=%s&amp;cl=false&amp;fromSearch=true&amp;labelWords=&amp;suginput=\" % info self.crawl_request(method=\"GET\", url=first_request_url) time.sleep(10) continue return response.text 将爬取到的数据保存到数据库：在以上我们爬取到的结果中，我们只是爬取了在result列表中的所有数据，可读性还比较差。我们需要创建一个数据库，并筛选出我们需要的数据插入进去。 创建数据库： 创建数据库： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#创建数据库连接engine = create_engine(\"mysql+pymysql://root:root@127.0.0.1:3306/lagou?charset=utf8\")#操作数据库Session = sessionmaker(bind=engine)#声明一个基类Base = declarative_base()class Lagoutables(Base): #表名称 __tablename__ = 'lagou_java' #id,设置为主键和自动增长 id = Column(Integer,primary_key=True,autoincrement=True) #职位id positionID = Column(Integer,nullable=True) # 经度 longitude = Column(Float, nullable=False) # 纬度 latitude = Column(Float, nullable=False) # 职位名称 positionName = Column(String(length=50), nullable=False) # 工作年限 workYear = Column(String(length=20), nullable=False) # 学历 education = Column(String(length=20), nullable=False) # 职位性质 jobNature = Column(String(length=20), nullable=True) # 公司类型 financeStage = Column(String(length=30), nullable=True) # 公司规模 companySize = Column(String(length=30), nullable=True) # 业务方向 industryField = Column(String(length=30), nullable=True) # 所在城市 city = Column(String(length=10), nullable=False) # 岗位标签 positionAdvantage = Column(String(length=200), nullable=True) # 公司简称 companyShortName = Column(String(length=50), nullable=True) # 公司全称 companyFullName = Column(String(length=200), nullable=True) # 工资 salary = Column(String(length=20), nullable=False) # 抓取日期 crawl_date = Column(String(length=20), nullable=False) 插入数据： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def __init__(self): self.mysql_session = Session() self.date = time.strftime(\"%Y-%m-%d\",time.localtime())#数据存储方法def insert_item(self,item): #今天 date = time.strftime(\"%Y-%m-%d\",time.localtime()) #数据结构 data = Lagoutables( #职位ID positionID = item['positionId'], # 经度 longitude=item['longitude'], # 纬度 latitude=item['latitude'], # 职位名称 positionName=item['positionName'], # 工作年限 workYear=item['workYear'], # 学历 education=item['education'], # 职位性质 jobNature=item['jobNature'], # 公司类型 financeStage=item['financeStage'], # 公司规模 companySize=item['companySize'], # 业务方向 industryField=item['industryField'], # 所在城市 city=item['city'], # 职位标签 positionAdvantage=item['positionAdvantage'], # 公司简称 companyShortName=item['companyShortName'], # 公司全称 companyFullName=item['companyFullName'], # 工资 salary=item['salary'], # 抓取日期 crawl_date=date ) #在存储数据之前查询表里是否有这条职位信息 query_result = self.mysql_session.query(Lagoutables).filter(Lagoutables.crawl_date==date, Lagoutables.positionID == item['positionId']).first() if query_result: print('该职位信息已存在%s:%s:%s' % (item['positionId'], item['city'], item['positionName'])) else: #插入数据 self.mysql_session.add(data) #提交数据 self.mysql_session.commit() print('新增职位信息%s' % item['positionId']) 运行结果： 此时职位信息已保存到数据库中： 完整代码：github：https://github.com/KeerZhou/crawllagoucsdn：https://download.csdn.net/download/keerzhou/11584694","link":"/2019/08/23/Python%E6%8B%89%E5%8B%BE%E7%BD%91%E7%88%AC%E8%99%AB%E5%B9%B6%E5%AD%98%E5%82%A8%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"HTTP协议简单理解","text":"一、HTTP协议简介1.HTTP概述 &emsp;&ensp;HTTP（Hyper Text Transfer Protocol，超文本传输协议），是一个属于应用层的面向对象的协议。用于从万维网（World Wide Web ）服务器传输超文本到本地浏览器的传送协议，客户端通过使用网页浏览器或者其它的工具发起一个HTTP请求到服务器上指定端口，服务器在收到请求之后，返回响应内容（文本、图片等）。可以说Web是建立在HTTP协议上通信的。 2.HTTP发展历程 HTTP/0.9&emsp;&ensp;HTTP于1990年问世，但那时HTTP并没有作为正式的标准被建立。HTTP/1.0&emsp;&ensp;HTTP正式作为标准被公布是在1996年5月，版本被命名为HTTP/1.0，并记载于RFC1945。HTTP 1.0引入了POST和HEAD命令，增强了交互功能，但规定浏览器与服务器只保持短暂的连接，连接成功后只能发送一次请求，然后连接就断开了，如果需要多次请求，需要多次连接。正式因为这种特性造成了一些性能上的缺陷。 HTTP/1.1&emsp;&ensp;1997年1月公布的HTTP/1.1是目前主流的HTTP协议版本，记载于RFC2616。HTTP 1.1支持持久连接，默认TCP连接不关闭，可以被多个请求复用，减少了建立和关闭连接的消耗和延迟，并且新增了更多的请求方式。HTTP/2.0&emsp;&ensp;2015年5月，HTTP/2发布，记载于RFC7540，主要解决了HTTP1.1的效率不高的问题，新增特性有：二进制协议、多工、数据流、头信息压缩等等功能。 二.HTTP基础——TCP/IP&emsp;&ensp;通常使用的网络是在TCP/IP协议族的基础上运作的，而HTTP协议属于它内部的一个子集。TCP/IP协议族按层次可分为：应用层、传输层、网络层和数据链路层。HTTP协议位于TCP/IP协议栈的应用层。&emsp;&ensp;发送端在层与层之间进行数据传输时，从应用层往下走，每经过一层就会被加上上一层的首部信息；接收端在层与层之间进行数据传输时，从数据链路层往上走，每经过一层，便会把相应的首部去掉。 三.HTTP报文&emsp;&ensp;用于HTTP协议交互的信息被称为HTTP报文。请求端（客户端）的HTTP报文叫做请求报文，响应端（服务器端）的叫做响应报文。 HTTP报文结构 通用首部字段 123456789Cache-Control：控制缓存行为Connection：链接的管理Date：报文日期Pragma：报文指令Trailer：报文尾部的首部Trasfer-Encoding： 指定报文主体的传输编码方式Upgrade：升级为其他协议Via：代理服务器信息Warning：错误通知 实体首部字段 12345678910Allow 资源可支持的HTTP方法Content-Encoding 实体的编码方式Content-Language 实体的自然语言Content-Length 实体的内容大小(字节为单位)Content-Location 替代对应资源的URIContent-MD5 实体的报文摘要Content-Range 实体的位置范围Content-Type 实体主体的媒体类型Expires 实体过期时间Last-Modified 资源的最后修改时间 请求报文请求方法 12345678910111213141516171819202122231.GET：获取资源GET方法向指定的资源发出请求。它本质就是发送一个请求来取得服务器上的某一资源。资源通过一组HTTP头和呈现数据（如HTML文本，或者图片或者视频等）返回给客户端。GET请求中，永远不会包含呈现数据。2.POST：传输实体主体POST方法向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。3.PUT：传输文件PUT方法用来传输文件，要求在请求报文的主体中包含文件内容，然后保存到请求URI指定的位置。4.HEAD：获得报文首部HEAD方法和GET方法一样，只是不返回报文主体部分。这一方法可以再不必传输整个响应内容的情况下，就可以获取包含在响应小消息头中的元信息。5.DELETE：删除文件DELETE方法用来删除文件，与PUT方法相反。DELETE方法按请求URL删除指定的资源。6.OPTIONS：询问支持的方法OPTIONS方法用来查询并返回服务器针对特定资源所支持的HTTP请求方法。7.TRACE：追踪路径TRACE方法是让Web服务器端将之前的请求通信返回给客户端的方法，主要用于测试或诊断。8.CONNECT：要求用隧道协议连接代理CONNECT方法要求在与代理服务器通信时建立隧道，实现用隧道协议进行TCP通信。 重点：GET方法和POST方法的区别 GET方法是从服务器上获取数据，POST方法是向服务器传送数据。 POST方法比GET方法安全，因为数据在地址栏上不可见。 GET方法提交的数据最多只能有1024字节，而POST方法则没有此限制。 请求首部字段 12345678910111213141516Client-IP：提供了运行客户端的机器的IP地址From：提供了客户端用户的E-mail地址Host：给出了接收请求的服务器的主机名和端口号Referer：提供了包含当前请求URI的文档的URLUA-Color：提供了与客户端显示器的显示颜色有关的信息UA-CPU：给出了客户端CPU的类型或制造商UA-OS：给出了运行在客户端机器上的操作系统名称及版本User-Agent：将发起请求的应用程序名称告知服务器 Accept：告诉服务器能够发送哪些媒体类型Accept-Charset：告诉服务器能够发送哪些字符集Accept-Encoding：告诉服务器能够发送哪些编码方式Accept-Language：告诉服务器能够发送哪些语言TE：告诉服务器可以使用那些扩展传输编码Expect：允许客户端列出某请求所要求的服务器行为Range：如果服务器支持范围请求，就请求资源的指定范围Cookie：客户端用它向服务器传送数据 响应报文状态码 &emsp;&ensp;状态码的作用是当客户端向服务器发送请求时，描述返回的请求结果，根据结果，用户可以知道服务器是正常处理了请求，还是出现了错误。 状态码的类别及常用状态码 100~199（信息性状态码） 100 Continue 继续。客户端应继续其请求。 101 Switching Protocols 切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议。 200~299（成功状态码） 200 OK 请求成功。表示从客户端发来的请求在服务器被正常处理了。一般用于GET与POST请求。 204 No Content 无内容。服务器成功处理，但在返回的响应报文中不含实体的主体部分。 206 Partial Content 部分内容。服务器成功处理了部分GET请求。 300~399（重定向状态码） 301 Moved Permanetly 永久性重定向。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替 302 Found 临时性重定向。该状态码表示请求的资源被分配了新的URI，希望用户本次能使用新的URI访问 303 See Other 查看其它地址。该状态码表示由于请求对应的资源存在着另一个URI，应使用GET方法定向获取请求的资源。 304 Not Modified 未修改。表示客户端发送附带条件的请求时，服务器端允许请求访问资源，但因发生请求未满足条件的情况后，直接返回304Not Modified，不包含任何响应的主体部分。 307 Temporary Redirect 临时性重定向。与302类似，307会遵照浏览器标准，不会从POST变成GET。 400~499（客户端错误状态码） 400 Bad Request 客户端请求的语法错误，服务器无法理解。 401 Unauthorized 请求要求用户的身份认证。 403 Forbidden 表明对请求资源的访问被服务器拒绝了。 404 Not Found 服务器无法根据客户端的请求找到资源。 500~599（服务器错误状态码） 500 Internal Server Error 服务器内部错误，无法完成请求 503 Service Unavailable 由于超载或系统维护，服务器暂时的无法处理客户端的请求。 响应首部字段 1234567891011Age：推算资源创建经过的时间Public：服务器为其资源支持的请求方法列表Retry-After：如果资源不可用的话，在此日期或时间重试Server：HTTP服务器的安装信息Title：对HTML文档来说，就是HTML文档的源端给出的标题Warning：比原因短语更详细一些的警告报文Accept-Ranges：对此资源来说，服务器可接受的范围类型Vary：代理服务器缓存的管理信息Proxy-Authenticate：来自代理的对客户端的质询列表Set-Cookie：在客户端设置数据，以便服务器对客户端进行标识WWW-Authenticate：服务器对客户端的认证信息 四.HTTP的缺点 通信使用明文（不加密），内容可能会被窃听。 不验证通信双方的身份，因此有可能遭遇伪装。 无法证明报文的完整性，所以可能已篡改。 HTTPSHTTPS = HTTP + 加密 + 认证 + 完整性保护 加密处理防止被窃听 &emsp;&ensp;HTTP协议中没有加密机制，但可以通过SSL(Secure Socket Layer，安全套接层)或TLS（Transport Layer Security，安全传输层协议）的组合使用，加密HTTP的通信内容。 查明对方的证书，确定通信方 &emsp;&ensp;HTTP协议无法确定通信方，但如果使用SSL则可以。SSL不仅提供加密处理，而且还使用了一种被称为证书的手段，用于确定通信方。 证明报文的完整性，防止篡改 &emsp;&ensp;最常用的是MD5和SHA-1等散列值校验的方法，以及用来确认文件的数字签名方法。 五.参考《图解HTTP》 [日]上野宣","link":"/2020/04/30/HTTP%E5%8D%8F%E8%AE%AE%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3/"}],"tags":[],"categories":[]}